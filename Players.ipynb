{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools \n",
    "import pickle\n",
    "from random import choice, randrange, uniform, randint\n",
    "from keras import models, layers, optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player \n",
    "This is the superclass used for all players. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Superclass Player, \n",
    "class Player: \n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.color = None\n",
    "        return \n",
    "    \n",
    "    def setcolor(self, color):\n",
    "        self.color = color\n",
    "        \n",
    "    def move(self):\n",
    "        if self.color is None:\n",
    "            print('Must assign color')\n",
    "            return \n",
    "        return\n",
    "    \n",
    "    def getgame(self):\n",
    "        return self.game\n",
    "    \n",
    "    def playvshuman(self, startplayer = 'bot'):\n",
    "        self.setcolor(1)\n",
    "        humancolor = -1\n",
    "\n",
    "        if startplayer == 'bot':\n",
    "            self.move()\n",
    "        while self.game.gamefinished() == False:\n",
    "            self.game.printboard()\n",
    "            if len(self.game.possiblesteps(humancolor)) != 0:\n",
    "                r = int(input(\"Row index \")) \n",
    "                c = int(input('Column index'))\n",
    "                \n",
    "                while self.game.turn((r,c)) == False\n",
    "                    cont = input('Can not pick', r,',', c,' press anywhere to enter new index')\n",
    "                    r = int(input(\"Row index \")) \n",
    "                    c = int(input('Column index'))\n",
    "                \n",
    "                self.game.printboard()\n",
    "            else:\n",
    "                print('No possible moves!')\n",
    "            cont = input('Press somewhere')\n",
    "            self.move()\n",
    "        self.game.printboard()\n",
    "        print('Game finished, winner: ', 'bot' if self.game.winner()== 1 else 'human')\n",
    "        return \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomplayer\n",
    "Randomly picks the next action from a list of all possible actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):       \n",
    "    def move(self):\n",
    "        super().move()\n",
    "        posmoves = self.game.possiblesteps(self.color)\n",
    "        if len(posmoves) != 0: \n",
    "            move = choice(posmoves)\n",
    "            self.game.turn(move, self.color)\n",
    "        return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability functions\n",
    "Functions below are used to calculate probability of win given a certain state (assuming all actions are random). i.e. if all possible combinations of actions are taken from a certain state, how often do they result in a win. These methods cannot actually be executed right now because they rely on methods that I changed and put in the Othello class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two functions below together calculates the probability that color will win, given that they make move. \n",
    "def probabilityofwin(board, move, color): \n",
    "    tempboard = np.copy(board) \n",
    "    turn(tempboard, move, color)\n",
    "    if gamefinished(tempboard):\n",
    "        return 1 if np.sign(np.sum(tempboard)) == np.sign(color) else 0\n",
    "    return probwinoppturn(tempboard, color)\n",
    "\n",
    "\n",
    "\n",
    "def probwinoppturn(board, owncolor):\n",
    "    posmovesopponent = possiblesteps(board, owncolor*-1)\n",
    "    winprobs = []\n",
    "    \n",
    "    if gamefinished(board):\n",
    "        return 1 if np.sign(np.sum(board)) == np.sign(owncolor) else 0\n",
    "    \n",
    "    if len(posmovesopponent) == 0: \n",
    "        posmoves = possiblesteps(board, owncolor)\n",
    "        for posmove in posmoves: \n",
    "            winprobs+= [probabilityofwin(board, posmove, owncolor)]\n",
    "        \n",
    "    else:\n",
    "        for oppmove in posmovesopponent: \n",
    "            tempboard = np.copy(board)\n",
    "            turn(tempboard, oppmove, owncolor*-1)\n",
    "            if gamefinished(tempboard):\n",
    "                winprobs += [1 if np.sign(np.sum(tempboard)) == np.sign(owncolor) else 0]\n",
    "            \n",
    "            else:\n",
    "                posmoves = possiblesteps(tempboard, owncolor)\n",
    "                if len(posmoves) == 0: \n",
    "                    winprobs += [probwinoppturn(tempboard, owncolor)]\n",
    "                else:\n",
    "                    for posmove in posmoves:\n",
    "                        winprobs += [probabilityofwin(tempboard, posmove, owncolor)]\n",
    "    return np.mean(winprobs)\n",
    "\n",
    "\n",
    "\n",
    "#Generating all possible boards starting from board, with firstcolor making the first move \n",
    "def genpossibleboards(board, firstcolor = 1, firstcall = True):\n",
    "    if gamefinished(board):\n",
    "        return [], []\n",
    "    posmoves = possiblesteps(board, firstcolor)\n",
    "    posboards = [turn(np.copy(board), posmove, firstcolor) for posmove in posmoves]\n",
    "    followingboards = []\n",
    "    colors = []\n",
    "    colors = np.append(colors, firstcolor*np.ones(len(posboards)))\n",
    "    for posboard in posboards: \n",
    "        temp = genpossibleboards(np.copy(posboard), firstcolor*-1, firstcall = False)\n",
    "        followingboards += temp[0]\n",
    "        colors = np.append(colors, temp[1])\n",
    "        if firstcall:\n",
    "            print(len(followingboards + posboards))\n",
    "    if len(posboards) == 0: \n",
    "        temp = genpossibleboards(np.copy(board), firstcolor*-1, firstcall = False)\n",
    "        followingboards += temp[0]\n",
    "        colors = np.append(colors, temp[1])\n",
    "        \n",
    "        if firstcall: \n",
    "            print(len(followingboards + posboards))\n",
    "    \n",
    "    return posboards + followingboards, colors\n",
    "\n",
    "\n",
    "\n",
    "#Calculating list of win probabilities for the boards and colors list specified.  \n",
    "def winprobs_for_boards(boards, colors): # the colors list specifies what color made the last move in a particular board\n",
    "    probs = [probwinoppturn(b, c) for (b,c) in zip(boards, colors)]\n",
    "    return probs\n",
    "\n",
    "# Calculates win probabilities for all possible boards of size boardsize\n",
    "def allpossibleboardsandprobs(boardsize):\n",
    "    board = createboard(boardsize)\n",
    "    boards, colors = genpossibleboards(board)\n",
    "    probs = winprobs_for_boards(boards, colors)\n",
    "    return boards, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProbabilityPlayer\n",
    "Uses brute force to calculate all possible outcomes, assign win probability for possible next actions and chooses next action to maximize win probability. Can use previously calculated win probabilities from file if path is specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilityPlayer(Player):\n",
    "    def __init__(self, game, probdict = None, probdict_file = None):\n",
    "        super().__init__(game)\n",
    "        if probdict != None:\n",
    "            self.probdict = probdict\n",
    "        elif probdict_file != None:\n",
    "            f = open(probdict_file,\"rb\")\n",
    "            self.probdict = pickle.load(f)\n",
    "            f.close()\n",
    "        else: \n",
    "            boards, probs = allpossibleboardsandprobs(boardsize = self.game.boardsize)\n",
    "            self.probdict = {}\n",
    "            for i in range(len(boards)): \n",
    "                board = boards[i]\n",
    "                Hash = str(board.reshape(len(board)*len(board))).replace('[', '').replace(']','').replace('.','').replace(' ', '')\n",
    "                self.probdict[Hash] = probs[i]\n",
    "        return \n",
    "    \n",
    "    def move(self):\n",
    "        posmoves = self.game.possiblesteps(self.color)\n",
    "        if len(posmoves) != 0:\n",
    "            posboards = [self.game.turn(move, self.color, keep_board = True) for move in posmoves]\n",
    "            posboards = [self.game.board_to_hash(board) for board in posboards]\n",
    "            probabilities = [self.probdict[board] for board in posboards]\n",
    "            index = np.argmax(probabilities)\n",
    "            move = posmoves[index]\n",
    "            self.game.turn(move, self.color)\n",
    "        return\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLPlayer\n",
    "Reinforcement learning player (epsilon-greedy). Keeps a table action_value_map to map string representations of boards to values (ranging between -win_value and win_value). Boards representing finished games will either have value win_value (if win), -win_value (if loose) or 0 (if draw). After a game is finished the values should be updating s.t. $$ V(a_{i})_{new} := V(a_{i})_{old} + lr*(V(a_{i+1})-V(a_{i})_{old}) $$ i.e. it will increase if it leads to higher ensuing values and decrease if it leads to lower ensuing values. Values are all initialized to starting_values. The Player will choose a random action a fraction of the time, specified by epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLPlayer(Player): \n",
    "    def __init__(self, game, epsilon = 0.1, starting_values = 0, win_value = 1, lr = 0.1):\n",
    "        super().__init__(game)\n",
    "        self.action_value_map = {self.game.board_to_hash() : starting_values}\n",
    "        self.train_epsilon = epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.starting_values = starting_values\n",
    "        self.win_value = win_value\n",
    "        self.actions = []\n",
    "        self.lr = lr\n",
    "        return \n",
    "    \n",
    "    def move(self):\n",
    "        posmoves = self.game.possiblesteps(self.color)\n",
    "        if len(posmoves) != 0: \n",
    "            pos_actions = [self.game.turn(move, self.color, keep_board = True) for move in posmoves]\n",
    "            pos_actions = [self.game.board_to_hash(board) for board in pos_actions]\n",
    "            \n",
    "            values = []\n",
    "            for action in pos_actions: \n",
    "                if action in self.action_value_map:\n",
    "                    values += [self.action_value_map[action]]\n",
    "                else:\n",
    "                    self.action_value_map[action] = self.starting_values\n",
    "                    values += [self.starting_values]\n",
    "            if uniform(0,1) > self.epsilon:\n",
    "                index = np.argmax(values)\n",
    "            else:\n",
    "                index = randrange(len(posmoves))\n",
    "            move = posmoves[index]\n",
    "            \n",
    "            self.actions += [pos_actions[index]]\n",
    "            self.game.turn(move, self.color)\n",
    "        return \n",
    "    \n",
    "    def update_values(self): \n",
    "        self.action_value_map[self.actions[-1]] = self.game.winner()*self.color*self.win_value \n",
    "        for index, action in reversed(list(enumerate(self.actions[0:-1]))):\n",
    "            prev = self.actions[index+1]\n",
    "            self.action_value_map[action] = self.action_value_map[action]*(1-self.lr) + self.lr*self.action_value_map[prev]\n",
    "        self.actions = []\n",
    "        \n",
    "    def test_mode(self):\n",
    "        self.epsilon = 0\n",
    "        return\n",
    "    def train_mode(self):\n",
    "        self.epsilon = self.train_epsilon\n",
    "        return\n",
    "    \n",
    "    def save_action_value_map(self, directory):\n",
    "        import pickle\n",
    "        with open(directory, 'wb') as f:\n",
    "            pickle.dump(self.action_value_map, f)\n",
    "        return \n",
    "    \n",
    "    def load_action_value_map(self, directory):\n",
    "        import pickle \n",
    "        with  open(directory, 'rb') as f:\n",
    "            self.action_value_map = pickle.load(f)\n",
    "        return \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNPlayer\n",
    "Similar to RLPlayer, but uses neural network to map boards (actions) to values ranging between -1 and 1. When a game is finished, all actions during the game will be assigned the highest value, and the network will perform one gradient descent step to fit these new action-value pairs. The idea is that the player will recognize a board even if it has not seen that specific board (because it has seen similar ones before), and have some perception of the value of the board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNPlayer(Player):\n",
    "    def __init__(self, game, epsilon = 0, lr = 0.0001, opt = 'adam'):\n",
    "        super().__init__(game)\n",
    "        self.network = models.Sequential()\n",
    "        self.network.add(layers.Conv2D(50,(2,2)))\n",
    "        self.network.add(layers.Conv2D(50,(2,2), padding = 'same', activation = 'relu'))\n",
    "        self.network.add(layers.Conv2D(50,(2,2), padding = 'same', activation = 'relu'))\n",
    "        self.network.add(layers.Flatten())\n",
    "        self.network.add(layers.Dense(1))\n",
    "        if opt == 'adam':\n",
    "            self.opt = optimizers.Adam(learning_rate = lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            self.opt = keras.optimizers.RMSprop(learning_rate= lr)\n",
    "        else:\n",
    "            self.opt = opt\n",
    "\n",
    "        self.network.compile(loss='mean_squared_error', optimizer = self.opt)\n",
    "        self.actions = []\n",
    "        self.epsilon = epsilon\n",
    "        self.train_epsilon = epsilon\n",
    "        return \n",
    "    \n",
    "    def move(self):\n",
    "        posmoves = self.game.possiblesteps(self.color)\n",
    "        if len(posmoves) != 0:\n",
    "            pos_actions = [self.game.turn(move, self.color, keep_board = True) for move in posmoves]\n",
    "            pos_actions = np.array(pos_actions).reshape((len(pos_actions), self.game.boardsize, self.game.boardsize, 1))\n",
    "            if uniform(0,1) > self.epsilon:\n",
    "                index = np.argmax(self.network.predict(pos_actions))\n",
    "            else:\n",
    "                index = randint(0, len(posmoves)-1)\n",
    "            move = posmoves[index]\n",
    "            self.actions += [pos_actions[index]]\n",
    "            self.game.turn(move, self.color)\n",
    "\n",
    "        return \n",
    "    \n",
    "    def update_values(self):\n",
    "        values = self.game.winner()*self.color*np.ones(len(self.actions))\n",
    "        self.actions = np.array(self.actions).reshape((len(self.actions), self.game.boardsize, self.game.boardsize, 1))\n",
    "        self.network.fit(self.actions, values, verbose = 0)\n",
    "        self.actions = []\n",
    "        return\n",
    "            \n",
    "    def test_mode(self):\n",
    "        self.epsilon = 0\n",
    "        return\n",
    "    \n",
    "    def train_mode(self):\n",
    "        self.epsilon = self.train_epsilon\n",
    "        return\n",
    "    \n",
    "    def change_opt(self, opt):\n",
    "        self.opt = opt \n",
    "        self.network.compile(loss='mean_squared_error', optimizer = self.opt)\n",
    "        return\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon \n",
    "        self.train_epsilon = epsilon\n",
    "        return\n",
    "    \n",
    "        \n",
    "    \n",
    "    def save_network(self, directory):\n",
    "        self.network.save(directory + '.h5')\n",
    "        return\n",
    "    \n",
    "    def load_network(self, directory):\n",
    "        self.network = models.load_model(directory + '.h5')\n",
    "        return \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNPlayer_mod\n",
    "Like the above, but instead of choosing action in order to maximize value, it chooses action from probabability distribution proportional to the values. i.e when choosing between action A with value 1, and action B with value 0, \n",
    "it will choose action A in 66% of the cases (as values range from -1 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNPlayer_mod(Player):\n",
    "    def __init__(self, game, epsilon = 0, lr = 0.0001, opt = 'adam'):\n",
    "        super().__init__(game)\n",
    "        self.network = models.Sequential()\n",
    "        self.network.add(layers.Conv2D(50,(2,2)))\n",
    "        self.network.add(layers.Conv2D(50,(2,2), padding = 'same', activation = 'relu'))\n",
    "        self.network.add(layers.Conv2D(50,(2,2), padding = 'same', activation = 'relu'))\n",
    "        self.network.add(layers.Flatten())\n",
    "        self.network.add(layers.Dense(1))\n",
    "        if opt == 'adam':\n",
    "            self.opt = optimizers.Adam(learning_rate = lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            self.opt = keras.optimizers.RMSprop(learning_rate= lr)\n",
    "        else:\n",
    "            self.opt = opt\n",
    "\n",
    "        self.network.compile(loss='mean_squared_error', optimizer = self.opt)\n",
    "        self.actions = []\n",
    "        self.epsilon = epsilon\n",
    "        self.train_epsilon = epsilon\n",
    "        return \n",
    "    \n",
    "    def move(self):\n",
    "        posmoves = self.game.possiblesteps(self.color)\n",
    "        if len(posmoves) != 0:\n",
    "            pos_actions = [self.game.turn(move, self.color, keep_board = True) for move in posmoves]\n",
    "            pos_actions = np.array(pos_actions).reshape((len(pos_actions), self.game.boardsize, self.game.boardsize, 1))\n",
    "            if uniform(0,1) > self.epsilon:\n",
    "                vals = self.network.predict(pos_actions)\n",
    "                if np.min(vals) < 0: #if the worst action has negative value, the value vector must be shifted befor normalizing\n",
    "                    vals += -1*np.min(vals)*np.ones(len(vals))\n",
    "                vals *= vals/np.sum(vals) # mapping values to probability distribution\n",
    "                index = choices(np.arange(0, len(posmoves)), vals)[0]\n",
    "            else:\n",
    "                index = randint(0, len(posmoves)-1)\n",
    "            move = posmoves[index]\n",
    "            self.actions += [pos_actions[index]]\n",
    "            self.game.turn(move, self.color)\n",
    "\n",
    "        return \n",
    "    \n",
    "    def update_values(self):\n",
    "        values = self.game.winner()*self.color*np.ones(len(self.actions))\n",
    "        self.actions = np.array(self.actions).reshape((len(self.actions), self.game.boardsize, self.game.boardsize, 1))\n",
    "        self.network.fit(self.actions, values, verbose = 0)\n",
    "        self.actions = []\n",
    "        return\n",
    "            \n",
    "    def test_mode(self):\n",
    "        self.epsilon = 0\n",
    "        return\n",
    "    \n",
    "    def train_mode(self):\n",
    "        self.epsilon = self.train_epsilon\n",
    "        return\n",
    "    \n",
    "    def change_opt(self, opt):\n",
    "        self.opt = opt \n",
    "        self.network.compile(loss='mean_squared_error', optimizer = self.opt)\n",
    "        return\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon \n",
    "        self.train_epsilon = epsilon\n",
    "        return\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
